# 人工智能的数学基础
![目录](images/人工智能的数学基础-目录.png)
补充：符号逻辑，图论
## 第一章 特征向量[^Eigenvector]与矩阵分析
#### 向量基础
[^Eigenvector]: <span style="color:red;">注：此处特征向量不等于线性代数中的特征向量,在机器学习和数据科学中，特征向量通常指一个样本的多维特征表示。</span>  

由单一数值构成的对待研究对象的量化评价，称作<span style="color:blue">标量</span>。标量的定义与其代表的数据类型强相关。
一个用于描述某个对象的​​多维度特征的有序集合​​称为<span style="color:blue">特征向量</span>。用[ ]标注。
由各个特征可能的取值张成的空间，称作“特征空间”。显然，特征空间限制了特征向量的取值范围。
$$\boldsymbol{x}=[x_1,x_2,x_3,...,x_d]$$
给定任一向量，其包含<span style="color:red">大小[^size]与方向</span>两类信息。
[^size]:我们常常使用两点间距离的平方和(**欧氏距离**)表示大小，即$\|\boldsymbol{x}\|=\sqrt{x_1^2+x_2^2+x_3^2+...+x_d^2}$或$\sqrt{\sum_{i=1}^dx_i^2}$但是请注意，大小不要局限于欧氏距离

$\boldsymbol{x_1}=[x_{1,1},x_{1,2},x_{1,3},...,x_{1,d}]\quad \boldsymbol{x_2}=[x_{2,1},x_{2,2},x_{2,3},...,x_{2,d}]$
相应地，只有分别在各分量位置处取相同值时，两个向量才相等（意味两个向量在空间中同一点）。$x_{1,i}=x_{2,i}$
几类特殊的向量：
零向量：$\boldsymbol{o}=[0,0,0,0,...,0]$
单位向量：$\boldsymbol{e}=\frac{\boldsymbol{x}}{\|\boldsymbol{x}\|}$
向量的转置：略
#### 向量运算
**向量加法**：向量加法是指将两个​​同维度​​的向量按照对应分量相加，得到一个新向量的运算。
**向量数乘**：数乘是指用一个标量（实数）乘以向量，每个分量都乘以该标量
**单位元**：在某种运算下，与任何元素结合都保持该元素不变。在线性代数中，向量的单位元是​​零向量​​。
**逆元**：对于向量加法，一个向量$\boldsymbol{u}$的逆元是它的​​**负向量**​​，记作$\boldsymbol{−u}$
**零元**：一个广义术语，泛指具有“归零”或“湮灭”性质的元素。在向量加法中，​​单位元​​是零向量 0,在标量乘法中，​​零向量​​是“零元”，即 0⋅$\boldsymbol{x=0}$,标量 0 是标量域中的零元，但不是向量空间的“零元”。
**向量内积**：对于两个维度相同的向量，它们的内积是其​​对应分量乘积之和​​，结果是一个​​标量​​（长度仅有一个维度向量，转置等于自身）。（满足交换律）
内积结果为0，说明两个向量正交，如果这两个向量均为单位向量，则称这种正交为*标准正交*。
若两个内积向量均已单位化，则向量内积可以作为两个*向量相似程度的判据*,单位向量的内积即为​​余弦相似度。长度确定情况下，内积越接近长度的乘积，则向量在方向上越相似。
用于改写给定函数的泰勒展开式+内积向量方向正好相反时，内积结果取最小值=*梯度降*
内积运算是行向量与列向量的乘积
**分类平面（决策边界）**：​​一个（或一组）用于将特征空间划分成不同类别区域的​​边界​​。与法向量同侧的向量为正，异侧为负。
设有平面方程：$ag+bh+cr+d=0$
扩展特征向量$\boldsymbol{x_c}=[g,h,r,1]\quad$权重向量$\boldsymbol{w}=[a,b,c,d]$
则该平面方程可以改写为向量形式$\boldsymbol{w·x_c^T=0}$
几何意义：$\boldsymbol{w·x_c^T}=0$是$ag+bh+cr+d=0$的等价向量表述形式，它们描述了同一个分类平面。权重向量$\boldsymbol{w}$的前三个分量$[a,b,c]$构成了分类平面的法向量，指出平面方向，即决策得分（$\boldsymbol{w·x^T}$）为正的一侧。
**向量外积**：向量外积是列向量与行向量的矩阵乘积。其运算结果是一个矩阵，该矩阵的每个元素都是列向量的一个分量与行向量的一个分量进行数乘（标量乘法）的结果。
**分量乘法（Hadamard积）**：分量乘法用$\odot$来表示，对于两个维度相同的向量：$\boldsymbol{w}=[w_1,w_2,w_3,...,w_d]\quad\boldsymbol{x}=[x_1,x_2,x_3,...,x_d]$它们的Hadamard积（分量乘法）定义为对应分量相乘，结果是一个新的同维向量：
$\boldsymbol{w\odot x}=[w_1x_1,w_2x_2,w_3x_3,...,w_dx_d]
$
一般地，特征分量对于分类或评分结果的贡献度不一定相同。Hadamard积提供了一种​​为每个特征分量施加不同权重​​的直接方式，从而体现各分量的差异化影响效果。
#### 向量线性相关性
任意向量，总能通过同一向量空间中其余向量得到。限定只包含**数乘与加法运算**————线性运算。
取向量空间中一组向量 $\boldsymbol{x_1,x_2,x_3,...,x_n}$，当且仅当标量值$a_1,a_2,...,a_n$均等于0，$a_1\boldsymbol{x}_1 + a_2\boldsymbol{x}_2 + \cdots + a_n\boldsymbol{x}_n = \boldsymbol{0}$才成立,则称$\boldsymbol{x_1,x_2,x_3,...,x_n}$为**线性无关**的，​​ 反之称这组向量为**线性相关**的，（如二维线性相关的两个向量共线，3维线性相关的两个向量共面）。
线性相关意味着其中至少有一个向量可以表示为其他向量的线性组合（即数乘与加法运算）。也就是说其中任意一个向量**不能**写成由其它向量的数乘与加法运算构成的线性组合时，该向量组为线性无关向量组。
如果对于向量空间中的一个线性无关向量组，不存在空间中另一个向量可以加入该向量组并保持向量组的线性无关，则该向量组为**极大线性无关组**能生成整个向量空间的向量集合，称为该空间的**​​生成组**​​。如果一个生成组中​​不包含任何冗余的向量​​（即去掉其中任何一个向量都无法再生成整个空间），则称它为​**​极小生成组**​​。
互不线性相关的最大集合是构成对应向量空间的向量的最小集合————**基向量**，基向量等价于同时满足极大线性无关组与极小生成组的向量。
#### 矩阵
**矩阵的定义**：一个 ​​m × n 矩阵​​ (Matrix) 是一个由​​数字​​（或更一般地，来自某个​​域​​的元素，如实数、复数）排成的 ​​m 行​​ (Row)、​​n 列​​ (Column) 的矩形阵列。
矩阵可以理解为对向量进行的线性变换。
向量是特殊的矩阵。
行数与列数对应相等的矩阵称作**同行矩阵**。同型且对应元素相等，则矩阵相等
行数与列数相等的矩阵，称作**方阵**。
零阵：
$$
\boldsymbol{O} = \begin{bmatrix}
    0 & 0 & \cdots & 0 \\
    0 & 0 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & 0
\end{bmatrix}
$$
对角阵$diag(\boldsymbol{x})$:
$$
\boldsymbol{D} = \begin{bmatrix}
    d_{1} & 0 & \cdots & 0 \\
    0 & d_{2} & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & d_{n}
\end{bmatrix}
$$
$$
\boldsymbol{D} = \operatorname{diag}(d_1, d_2, \ldots, d_n)
$$
单位阵（$E$）：
$$
\boldsymbol{E}=\begin{bmatrix}
    1&0&\cdots&0\\
    0&1&\cdots&1\\
    \vdots&\vdots&\ddots&\vdots\\
    0&0&\cdots&1
\end{bmatrix}
$$
矩阵的基本运算:
矩阵加法/减法​​：只有​​同维度​​的矩阵才能相加减。规则是​​对应元素相加减​​。
标量乘法​​：用一个数（标量）乘以矩阵中的​​每一个元素​​。
​​矩阵乘法​​：这是最重要但也最需要理解的运算。
&emsp;一个​m×n​​的矩阵$​​A$​​和一个​n×p​​的矩阵​$​B$​​可以相乘,结果是一个​m×p​的矩阵​$​C$​​。
&emsp;$C​​$中第i行第j列的元素$c_{ij}$ ，等于$​​A$​​的第i行$​​B​$​的第j列的​​点积（内积）​​。
&emsp;重要​​：矩阵乘法​​不满足交换律​​，即AB$\neq$BA。
矩阵的转置：
一个​​m×n​​矩阵$​​A$​​的​​转置​​，记作$A^T$或​​A​​'，是一个​​n×m​​矩阵。
矩阵加法与数乘统称为矩阵的线性运算。
若无特殊说明，矩阵内积乘法时，不再特殊标注左操作数的列数与右操作数的行数，而是**默认二者相等**。
矩阵的乘法满足结合律和分配律，但是不满足交换律。
$AO=OA=O$&emsp;零元
$AE=EA=E$&emsp;单位元
$AB=BA=E$&emsp;逆元
矩阵的幂：
$A^{k+l}=A^kA^l$ &emsp; (前提：$A$是方阵)
$(AB)^k\neq A^kB^l$
$(AB)^T=B^TA^T$
矩阵内积等于向量外积的和——**内积的外积展开**。
矩阵乘法（基于内积）可以分解为多个外积矩阵的求和，矩阵乘积$AB$（由内积计算）等于所有列向量和行向量外积的和。
设$A$是$m×n$矩阵，矩阵$B$是$n×p$矩阵。它们的乘积$C=AB$是一个$m×p$矩阵。
外积展开形式：$$AB=\sum_{k=1}^{n}a_kb_k^T$$
其中：
&emsp;&emsp;&emsp;$a_k$是$A$的第$k$列（一个$m×1$列向量），
&emsp;&emsp;&emsp;$b_k^T$是$B$的第$k$行（一个$1×p$行向量），
&emsp;&emsp;&emsp;$a_kb_k^T$是一个$m×p$矩阵（即外积）。
**矩阵元素相乘**：矩阵的​​元素相乘​​（Element-wise Multiplication），也称为 ​Hadamard积​​（Hadamard Product）或 ​​Schur积​​（Schur Product），是一种二元运算。
核心规则：两个​​同维度​​的矩阵（即行数和列数完全相同）才能进行元素相乘。结果是一个新的​​同维度矩阵​​，其每个元素的值是原始两个矩阵​​对应位置元素的乘积​​。通常用$\odot$符号表示。
#### 矩阵的特征值与特征向量
**乘法形式：左乘与右乘**[^multiplicative_form]
矩阵与向量的乘法有两种等价的表示形式，取决于将向量视为列向量还是行向量。
右乘（列向量视角）
​$\boldsymbol{y=Ax}$
矩阵$A$右乘于列向量$x$，得到新的列向量$y$。（这是线性代数和科学技算中最常用的形式）
左乘（行向量视角）
​$\boldsymbol{y^T=x^TA}$，得到新的列向量$y^T$。

[^multiplicative_form]:两种形式描述的变换本质相同，只是表示惯例的差异。通常优先使用​​右乘列向量​​的形式。

**矩阵变换的几何效果**:矩阵对向量的乘法是一种​**​线性变换**​​，其效果取决于矩阵的形状。
非方阵 $(m×n,m\ne n)​$
变换是一种​​投影或升降维​​操作。变换后的新向量$y$与原向量$x$的​​维度不同$(m\ne n)$，因此直接比较“长度”(参考注释2)意义不大。
方阵$(n×n)​$
变换是在​​同一空间内​​的变换（输入输出维度相同）。
几何效果可理解为对原向量$x$进行​​旋转​​和​​缩放​​的组合，从而得到新向量$y$。
**特征向量与特征值：变换中的“不变量”**
对于方阵$A$，存在一些特殊的向量，在线性变换中保持方向不变。
定义（右特征向量/值）​​：
若存在一个​​非零​​列向量v和一个标量$\lambda$，使得：$$Av=\lambda v$$
则称：
&emsp;&emsp;&emsp;$v$是矩阵$A$的一个​​特征向量​​。
&emsp;&emsp;&emsp;$\lambda$是该特征向量对应的​​特征值​​。
几何解释​：
&emsp;&emsp;特征向量$v$在经过矩阵$A$的变换后，​​方向保持不变​​。
&emsp;&emsp;特征值$\lambda$量化了变换过程中沿该方向​​缩放的大小。
$|\lambda|>1:被拉伸\\
|\lambda|=1:长度不变\\
|\lambda|<1:被压缩\\
\lambda<0:方向反向$
左特征向量/值同理
重要性质：
&emsp;&emsp;1.非零缩放​​：任何非零标量$k$乘以特征向量($kv$)后，​​仍然是​​同一个特征值$\lambda$对应的特征向量。
&emsp;&emsp;2.单位特征向量​​：为了比较的方便，常将特征向量​​标准化​​为单位长度（模长为1）。
&emsp;&emsp;3.计算简化​​：特征向量将矩阵乘法运算$Av$​​简化为​​数乘运算$\lambda v$，极大地简化了分析和计算。
**特征降维（待深入学习，目前理解程度低）**：特征降维是一种通过数学变换，将高维数据映射到一个低维空间的技术，其目标是保留数据中最重要的结构信息（如方差、区分度），同时摒弃冗余、噪声和无关细节。
​特征向量指明了​​最重要的方向​​，特征值告诉我们这些方向​​有多重要​​。
**用特征降维简化运算**
降维目标：降维后各维度方差尽可能大，保证不同维度之间的相关性为0（基向量正交）<span style="color: #6c757d; font-size: 0.9em; font-style: italic;">
💡 也就是协方差尽可能小
</span>

<span style="color: #6c757d; font-size: 0.9em; font-style: italic;">
💡 课上这里比较跨越，我们先来了解一下协方差的知识
</span>

**协方差**
协方差是衡量​​两个随机变量​​（例如变量 X和 Y）之间​​线性相关关系​​的方向和强度的统计量。
若存在两个随机变量$X$和$Y$，其期望值（均值）分别为$\mu_x$和$\mu_y$，则它们的协方差定义为：$$cov(X,Y)=E[(X-\mu_x)(Y-\mu_y)]$$
​对于一组含有$n$个样本的数据，其样本协方差的计算公式为：
$$S_{xy}=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar x)(y_i-\bar y)$$
几何解释​​：
将每个数据点$(x_i,y_i)$视为一个高维空间中的向量（在二维情况下就是一个点）。
公式中的$(x_i-\bar x)和(y_i-\bar y)$表示该点相对于数据中心$(\bar x,\bar y)$的​**​偏差**​​。
协方差本质上是计算这些​​偏差向量​​在四个象限中的分布情况，并求其平均乘积。
​​协方差 > 0 (正)​​：数据点主要分布在第一、三象限。表明当一个变量取值高于其均值时，另一个变量也倾向于高于其均值。两个变量之间存在​​正相关​​趋势。其散点图呈“右上-左下”的椭圆形分布。
​​协方差 < 0 (负)​​：数据点主要分布在第二、四象限。表明当一个变量取值高于其均值时，另一个变量倾向于低于其均值。两个变量之间存在​​负相关​​趋势。其散点图呈“左上-右下”的椭圆形分布。
​​协方差 = 0​​：数据点在各象限均匀分布，无明显的线性趋势。两个变量​​线性不相关​​。
**从协方差到协方差矩阵​​**
当处理多个变量（$p$个）时，我们将所有变量两两之间的协方差排列成一个矩阵，称为​​协方差矩阵$​​\sum$（或$ S$）
**协方差矩阵**
对于一个以向量 $\mathbf{X} = [X_{1}, X_{2}, ..., X_{p}]^{T}$ 表示的随机变量，其协方差矩阵是一个方阵，其中的每个元素 $\Sigma_{ij}$ 是变量 $X_{i}$ 和 $X_{j}$ 之间的协方差。
$$
\Sigma = \begin{bmatrix}
\text{cov}(X_{1}, X_{1}) & \text{cov}(X_{1}, X_{2}) & \cdots & \text{cov}(X_{1}, X_{p}) \\
\text{cov}(X_{2}, X_{1}) & \text{cov}(X_{2}, X_{2}) & \cdots & \text{cov}(X_{2}, X_{p}) \\
\vdots & \vdots & \ddots & \vdots \\
\text{cov}(X_{p}, X_{1}) & \text{cov}(X_{p}, X_{2}) & \cdots & \text{cov}(X_{p}, X_{p})
\end{bmatrix}
$$
重要性质
1. **方阵与对称性**: 协方差矩阵是一个对称方阵 $(p \times p)$，因为 $\text{cov}(X_{i}, X_{j}) = \text{cov}(X_{j}, X_{i})$。
2. **对角线元素**: 对角线上的元素 $\Sigma_{ii} = \text{cov}(X_{i}, X_{i}) = \text{Var}(X_{i})$ 是变量 $X_{i}$ 自身的方差。
3. **半正定性**: 协方差矩阵是一个半正定矩阵，这意味着其所有特征值均 $\geq 0$。这一性质至关重要。
协方差矩阵的特征分解：揭示主方向
协方差矩阵作为方阵，可以进行特征分解，其几何意义非常深刻。
**定义（特征分解）**:
$$ \Sigma \mathbf{v}_i = \lambda_i \mathbf{v}_i $$
其中：
- $\mathbf{v}_i$ 是矩阵 $\Sigma$ 的一个**特征向量**。
- $\lambda_i$ 是该特征向量对应的**特征值**。

**几何解释**:
- 协方差矩阵 $\Sigma$ 可以看作一个对数据点云进行**线性变换**的操作。
- **特征向量 $\mathbf{v}_i$** 指明了数据分布的主要**方向**（主方向）。
- **特征值 $\lambda_i$** 量化了数据在对应特征向量方向上的**离散程度（方差）**。$\lambda_i$ 越大，表示数据在该方向上的伸展越长，包含的信息越多。
重要性质
1.  **主成分分析 (PCA) 的基础**: PCA的本质就是求取协方差矩阵的特征值和特征向量。最大的特征值对应的特征方向就是**第一主成分**，即数据方差最大的方向。
2.  **标准化**: 特征向量通常被**标准化**为单位长度，以便比较不同方向上的方差（特征值）。
3.  **正交性**: 由于协方差矩阵是对称矩阵，其特征向量之间是**相互正交**的，构成了数据空间的一组正交基。
总结与应用
**协方差**: 衡量两个变量间的线性相关性。
**协方差矩阵**: 系统性地表示多个变量两两之间的协方差关系，是描述数据集形状（分布方向）的核心。
**特征值/特征向量**: 揭示了数据集内在的、不相关的**主方向**（特征向量）及其在各方向上的**扩展幅度**（特征值）。
**应用**:
主成分分析 (PCA)、线性判别分析 (LDA)、卡尔曼滤波、马氏距离等众多统计与机器学习算法都建立在协方差矩阵及其性质之上。
<span style="color: #6c757d; font-size: 0.9em; font-style: italic;">
💡 本段有些晦涩，不妨再来研究一下主成分分析法(PCA)
</span>

##### PCA (主成分分析)

一种降维与特征提取方法。旨在找到一组新的正交坐标轴（**主成分**），以重新表述数据集，使得数据在新坐标轴上的投影**方差**依次最大化。
<span style="color: #6c757d; font-size: 0.9em; font-style: italic;">
💡 想象坐标轴上分布着散乱的点，每个点代表一个样本，PCA正是让这些点尽可能落在新坐标轴的方法。
</span>

**关键步骤与原理**
1. 数据预处理：中心化
将原始数据矩阵 $\boldsymbol{X}$ ($m$ 个样本, $n$ 个特征) 中心化，得到均值向量为 $\boldsymbol{0}$ 的新矩阵 $\boldsymbol{X'}$。
$$\boldsymbol{X'} = \boldsymbol{X} - \boldsymbol{\mu}$$
<span style="color: #6c757d; font-size: 0.9em; font-style: italic;">
💡 相当于移动原坐标轴的原点到散乱的点群中心，中心化其实就是换了一个中心
</span>

2. 构建协方差矩阵
计算中心化数据 $\boldsymbol{X'}$ 的协方差矩阵 $\boldsymbol{\Sigma}$，以衡量特征间的共同变化趋势。
$$\boldsymbol{\Sigma} = \frac{1}{m-1} \boldsymbol{X'}^T\boldsymbol{X'}$$
- $\boldsymbol{\Sigma}$ 是一个 $n \times n$ 的实对称方阵。

3. 特征分解：寻找主成分
对 $\boldsymbol{\Sigma}$ 进行特征分解，求解特征方程：
$$\boldsymbol{\Sigma v} = \lambda \boldsymbol{v}$$
- **特征向量 $\boldsymbol{v}_i$**：方向，即**主成分**。指向数据方差最大的方向。
- **特征值 $\lambda_i$**：大小，量化了数据在对应主成分方向上的**方差**。

4. 排序与选择
将特征对按特征值**从大到小**排序：
$$(\lambda_1, \boldsymbol{v}_1), (\lambda_2, \boldsymbol{v}_2), ..., (\lambda_n, \boldsymbol{v}_n)$$
- $\boldsymbol{v}_1$ 为第一主成分（方差最大），$\boldsymbol{v}_2$ 为第二主成分，依此类推。

5. 降维变换（投影）
选取前 $k$ 个主成分组成投影矩阵 $\boldsymbol{W}$：
$$\boldsymbol{W} = [\boldsymbol{v}_1, \boldsymbol{v}_2, ..., \boldsymbol{v}_k]$$
将中心化数据 $\boldsymbol{X'}$ **右乘**于 $\boldsymbol{W}$，得到降维后的数据 $\boldsymbol{Y}$ ($m \times k$)：
$$\boldsymbol{Y} = \boldsymbol{X'} \boldsymbol{W}$$

**重要性质**
- **正交性**：主成分（特征向量）之间相互正交。
- **方差解释度**：前 $k$ 个主成分的方差贡献率为：
  $$\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^n \lambda_i}$$
- **最优性**：在最小化重构误差和最大化投影方差的意义下，PCA是最优的降维方法。

**本质**
对数据协方差矩阵进行特征分解，通过保留最大方差对应的特征向量方向，实现数据的有效降维与信息保留。